<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ä¸€ä¸ªå’¸é±¼çš„ç¬”è®°">
<meta property="og:type" content="website">
<meta property="og:title" content="QUANTNOTE">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="QUANTNOTE">
<meta property="og:description" content="ä¸€ä¸ªå’¸é±¼çš„ç¬”è®°">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ryan Liu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>QUANTNOTE</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">QUANTNOTE</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/25/Stochastic%20Calculus/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ryan Liu">
      <meta itemprop="description" content="ä¸€ä¸ªå’¸é±¼çš„ç¬”è®°">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QUANTNOTE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/25/Stochastic%20Calculus/" class="post-title-link" itemprop="url">Stochastic Calculus for Finance</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-25 00:00:00" itemprop="dateCreated datePublished" datetime="2022-01-25T00:00:00+08:00">2022-01-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-24 21:33:16" itemprop="dateModified" datetime="2022-02-24T21:33:16+08:00">2022-02-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">å­¦ä¹ ç¬”è®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


<h1 id="General-Probability-Theory"><a href="#General-Probability-Theory" class="headerlink" title="General Probability Theory"></a>General Probability Theory</h1><h2 id="infinite-probability-spaces"><a href="#infinite-probability-spaces" class="headerlink" title="infinite probability spaces"></a>infinite probability spaces</h2><h3 id="sigma-ä»£æ•°"><a href="#sigma-ä»£æ•°" class="headerlink" title="$\sigma$-ä»£æ•°"></a>$\sigma$-ä»£æ•°</h3><p>$\sigma-algebra$, F is a collection of subsets of $\Omega$</p>
<ol>
<li>empty set belongs to it</li>
<li>whenever a set belongs to it, its complement also belong to F</li>
<li>the union of a sequence of sets in F belongs to F</li>
</ol>
<p>å¯ä»¥æœ‰æ¨è®ºï¼š</p>
<ol>
<li>$\Omega$ ä¸€å®šåœ¨Fä¸­</li>
<li>é›†åˆåºåˆ—çš„äº¤é›†ä¹Ÿä¸€å®šåœ¨Fä¸­</li>
</ol>
<h3 id="æ¦‚ç‡æµ‹é‡çš„å®šä¹‰-probability-measure"><a href="#æ¦‚ç‡æµ‹é‡çš„å®šä¹‰-probability-measure" class="headerlink" title="æ¦‚ç‡æµ‹é‡çš„å®šä¹‰ probability measure"></a>æ¦‚ç‡æµ‹é‡çš„å®šä¹‰ probability measure</h3><p>ä¸€ä¸ªå‡½æ•°ï¼Œå®šä¹‰åŸŸæ˜¯Fä¸­çš„ä»»æ„é›†åˆï¼Œå€¼åŸŸæ˜¯[0,1]ï¼Œæ»¡è¶³ï¼š</p>
<ol>
<li><p>$P(\Omega)&#x3D;1$</p>
</li>
<li><p>$P(\cup^\infty_{n&#x3D;1} A_n)&#x3D;\sum^\infty_{n&#x3D;1} P(A_n)$</p>
<p> $triple(\Omega,F,P)$ç§°ä¸ºæ¦‚ç‡ç©ºé—´</p>
</li>
</ol>
<p>uniformï¼ˆLebesgueï¼‰measure on [0,1]ï¼šåœ¨0ï¼Œ1ä¹‹é—´é€‰å–ä¸€ä¸ªæ•°ï¼Œå®šä¹‰ï¼š</p>
<p>$P(a,b)&#x3D;P[a,b]&#x3D;b-a,0\le a \le b\le 1$</p>
<p>å¯ä»¥ç”¨ä¸Šè¿°æ–¹å¼è¡¨è¿°æ¦‚ç‡çš„é›†åˆçš„é›†åˆæ„æˆäº†sigmaä»£æ•°ï¼Œä»åŒ…å«çš„æ‰€æœ‰é—­åŒºé—´å‡ºå‘ï¼Œç§°ä¸ºBorel sigmaä»£æ•°</p>
<p>$(a,b)&#x3D;\cup_{n&#x3D;1}^\infty[a+\frac{1}{n},b-\frac{1}{n}]$ ï¼Œæ‰€ä»¥sigmaä»£æ•°ä¸­åŒ…å«äº†æ‰€æœ‰å¼€åŒºé—´ï¼Œåˆ†åˆ«å–è¡¥é›†å¯ä»¥å¯¼å‡ºåŒ…å«å¼€åŒºé—´ä¸é—­åŒºé—´çš„å¹¶ï¼Œä»è€Œå¯¼å‡ºæ‰€æœ‰é›†åˆ  </p>
<p>é€šè¿‡ä»é—­åŒºé—´å‡ºå‘ï¼Œæ·»åŠ æ‰€æœ‰å¿…è¦å…ƒç´ æ„æˆçš„sigmaä»£æ•°ç§°ä¹‹ä¸º$Borel \quad\sigma-algebra$ of subsets of [0,1] and is denoted B[0,1]</p>
<p>event A occurs almost surely if P(A)&#x3D;1</p>
<h2 id="Random-variables-and-distributions"><a href="#Random-variables-and-distributions" class="headerlink" title="Random variables and distributions"></a>Random variables and distributions</h2><p>definition: a random variable is a real-valued function X defined on $\Omega$  with the property that  for every Borel subset B of <strong>R,</strong> the subset of $\Omega$  given by:</p>
<p>$$<br>\begin{equation*}{x\in B}&#x3D;{\omega \in \Omega ; X(\omega )\in B}\end{equation*}<br>$$</p>
<p>is in the $\sigma$-algebra F</p>
<p>æœ¬è´¨æ˜¯æŠŠäº‹ä»¶æ˜ å°„ä¸ºå®æ•°ï¼ŒåŒæ—¶ä¸ºäº†ä¿è¯å¯ä»¥æ‹Ÿæ˜ å°„ï¼Œè¦æ±‚å‡½æ•°å¯æµ‹ã€‚</p>
<p>æ„é€ Rçš„Borelå­é›†ï¼Ÿä»æ‰€æœ‰çš„é—­åŒºé—´å‡ºå‘ï¼Œé—­åŒºé—´çš„äº¤â€”â€”ç‰¹åˆ«åœ°ï¼Œå¼€åŒºé—´ä¹ŸåŒ…å«è¿›æ¥ï¼Œä»è€Œå¼€é›†åŒ…å«è¿›æ¥ ï¼Œå› ä¸ºæ¯ä¸ªå¼€é›†å¯ä»¥å†™æˆå¼€åŒºé—´åºåˆ—çš„å¹¶ã€‚é—­é›†ä¹Ÿæ˜¯Borelé›†åˆï¼Œå› ä¸ºæ˜¯å¼€é›†çš„è¡¥é›†ã€‚</p>
<p>å…³æ³¨Xå–å€¼åŒ…å«äºæŸé›†åˆè€Œä¸æ˜¯å…·ä½“çš„å€¼</p>
<p>Definition: let X be a random variable on a probability space, the distribution measure of X is the probability measure $\mu _X$that assigns to each Borel subset B of R the mass $\mu _X(B)&#x3D;P{X\in B}$ </p>
<p>Random variable æœ‰distribution ä½†ä¸¤è€…ä¸ç­‰åŒï¼Œä¸¤ä¸ªä¸åŒçš„Random variable å¯ä»¥æœ‰ç›¸åŒçš„distributionï¼Œä¸€ä¸ªå•ç‹¬çš„random variable å¯ä»¥æœ‰ä¸¤ä¸ªä¸åŒçš„distribution</p>
<p>cdfï¼š$F(x)&#x3D;P{X\le x}$</p>
<p>$\mu_X(x,y]&#x3D;F(y)-F(x)$</p>
<h2 id="Expectations"><a href="#Expectations" class="headerlink" title="Expectations"></a>Expectations</h2><p>$E(X)&#x3D;\sum X(\omega )P(\omega )$</p>
<p>${\Omega , F, P}$ random variable $X(\omega )$ Pæ˜¯æ¦‚ç‡ç©ºé—´ä¸­çš„æµ‹åº¦</p>
<p>$A_k&#x3D;{\omega \in \Omega ; y_k\le X(\omega ) &lt; y_{k+1}}$</p>
<p>lower Lebesgue sum $LS^-_{\Pi}&#x3D;\sum y_k P(A_k)$</p>
<p>the maximal distance between the $y_k$ partition points approaches zero, we get Lebesgue integral$\int_{\Omega}X(\omega )dP(\omega)$</p>
<p>Lebesgue integralç›¸å½“äºæŠŠç§¯åˆ†æ¦‚å¿µæ‹“å±•åˆ°å¯æµ‹ç©ºé—´ï¼Œè€Œä¸æ˜¯å•çº¯çš„æ›´æ¢äº†æ±‚å’Œæ–¹å¼ã€‚æ¨ªåæ ‡å®é™…ä¸Šæ˜¯$\Omega$çš„æµ‹åº¦ã€‚</p>
<p>if the random variables X can take both positive and negative values, we can define the positive and negative parts of X:</p>
<p>$X^+&#x3D;max{X,0}$, $X^-&#x3D;min{-X,0}$</p>
<p>$\int XdP&#x3D;\int X^+dP-\int X^- dP$</p>
<p><strong>Comparison</strong></p>
<p>If $X\le Y$ almost surely, and if the Lebesgue integral are defined, then</p>
<p>$\int _{\Omega}X(\omega)dP(\omega)\le \int _{\Omega}Y(\omega)dP(\omega)$</p>
<p>If $X&#x3D;Y$ almost surely, and if the Lebesgue integral are defined, then</p>
<p>$\int _{\Omega}X(\omega)dP(\omega)&#x3D;\int _{\Omega}Y(\omega)dP(\omega)$</p>
<p><strong>Integrability, Linearity â€¦</strong></p>
<p><strong>Jensenâ€™s inequality</strong></p>
<p>if $\phi$ is a convex, real-valued function defined on R and if $E|X|&lt;\infty$ ,then</p>
<p>$\phi(EX)\le E\phi(X)$</p>
<p>$\mathcal{B} (\mathbb{R})$ be the sigma-algebra of Borel subsets of $\mathbb{R}$, the Lebesgue measure $\mathcal{L}$ on R assigns to each set $B\in \mathcal{B}(\mathbb{R})$ a number in $[0,\infty)$ or the value $\infty$ so that:</p>
<ol>
<li>$\mathcal{L}[a,b] &#x3D;b-a$ </li>
<li>if $B_1,B_2,B_3 \dots$  is a sequence of disjoint sets in $\mathcal{B}$ ,then we have the countable additivity property: $\mathcal{L} (\cup_{n&#x3D;1}^\infty B_n)&#x3D;\sum_{n&#x3D;1}^\infty \mathcal{L} (B_n)$</li>
</ol>
<p>é»æ›¼ç§¯åˆ†æœ‰ä¸”åªæœ‰åœ¨åŒºé—´ä¸­éè¿ç»­ç‚¹çš„é›†åˆçš„Lebesgueæµ‹åº¦ä¸ºé›¶æ—¶æœ‰å®šä¹‰ï¼Œå³fåœ¨åŒºé—´ä¸Šå‡ ä¹å¤„å¤„è¿ç»­</p>
<p>è‹¥fçš„Riemannç§¯åˆ†åœ¨åŒºé—´ä¸Šå­˜åœ¨ï¼Œåˆ™fæ˜¯Borelå¯æµ‹çš„ï¼Œè€Œä¸”Riemannç§¯åˆ†å’ŒLebesgueç§¯åˆ†ä¸€è‡´ã€‚</p>
<h3 id="convergence-of-integrals"><a href="#convergence-of-integrals" class="headerlink" title="convergence of integrals"></a>convergence of integrals</h3><p><strong>definition</strong></p>
<p>$X_1,X_2,X_3\dots$ be a sequence of random variables on the same probability space$(\Omega, \mathcal{F},\mathbb{P})$, $X$ be another random variable. $X_1,X_2,X_3\dots$  converges to $X$ almost surely $lim_{n\to \infty}X_n&#x3D;X$  almost surely. if the set of $\omega \in \Omega$ for the sequence has limit $X(\omega)$ is a set with probability one. </p>
<p>Strong law of Large Numbers: </p>
<p>å®çš„Borelå¯æµ‹çš„å‡½æ•°åˆ—$f_1,f_2,f_3\dots$ defined on$\mathbb{R}$, $f$ ä¹Ÿæ˜¯å®çš„Borelå¯æµ‹å‡½æ•°ï¼Œthe sequence converges to f almost every-where if åºåˆ—æé™ä¸ä¸ºfçš„ç‚¹çš„é›†åˆçš„ Lebesgue measureä¸ºé›¶</p>
<p>$lim_{n\to \infty}f_n&#x3D;f \textit{      almost everywhere}$</p>
<p>å½“éšæœºå˜é‡å‡ ä¹è¶‹äºä¸€è‡´ï¼Œä»–ä»¬çš„æœŸæœ›å€¼è¶‹äºæé™çš„æœŸæœ›ã€‚ç±»ä¼¼åœ°ï¼Œå½“å‡½æ•°å‡ ä¹å¤„å¤„convergeï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œä»–ä»¬çš„lebesgueç§¯åˆ†æ”¶æ•›åˆ°æé™çš„ç§¯åˆ†ã€‚ç‰¹æ®Šçš„æƒ…å†µï¼Œå°±æ˜¯</p>
<p>$lim_{n\to \infty}\int f_n(x)dx\ne \int lim_{n\to \infty} f_n(x)dx$</p>
<p>ä¾‹å¦‚fæ˜¯æ­£æ€åˆ†å¸ƒï¼Œå·¦è¾¹ä¸º1ï¼Œå³è¾¹lebesgueç§¯åˆ†ä¸º0</p>
<p><strong>theorem</strong> <em>Monotone convergence</em> $X_1,X_2,X_3\dots$ be a sequence of random variables converging almost surely to another random variable $X$ if åºåˆ—å‡ ä¹å•è°ƒä¸å‡ï¼ŒæœŸæœ›çš„æé™æ˜¯EX</p>
<p>$lim_{n \to \infty}\mathbb{E}X_n&#x3D;\mathbb{E} X$</p>
<p>æŠŠè¿™é‡Œçš„éšæœºå˜é‡æ¢æˆBorelå¯æµ‹å®å€¼å‡½æ•°ï¼Œä¹Ÿæ˜¯æˆç«‹çš„</p>
<p>$lim_{n\to \infty}\int_{-\infty}^{\infty}f_n(x)dx&#x3D;\int_{-\infty}^{\infty}f(x)dx$</p>
<p>å³ä½¿éšæœºå˜é‡å‡ ä¹ä¸å‘æ•£ï¼Œä½†æœŸæœ›å¯èƒ½å‘æ•£</p>
<p><strong>theorem Dominated convergence</strong> å¦‚æœéšæœºå˜é‡åºåˆ—å‡ ä¹è¶‹äºä¸€è‡´äº$X$,ä¸”æ»¡è¶³ $|X_n|\le Y$ almost surely for every n, $\mathbb{E}Y&lt;\infty$, then $lim_{n\to \infty}EX_n&#x3D;EX$</p>
<p>å¯¹äºBorelå¯æµ‹å®å€¼å‡½æ•°ä¹Ÿä¸€æ ·æˆç«‹ï¼šè‹¥$f_n(x)\le g$ almost surely for every n, and $\int_{-\infty}^\infty g(x)dx&lt;\infty$</p>
<p>è‹¥$f_n(x)\le g$ almost surely for every n, and $\int_{-\infty}^\infty g(x)dx&lt;\infty$</p>
<p>$lim_{n\to \infty}\int_{-\infty}^\infty f_n(x)dx&#x3D;\int_{-\infty}^\infty f(x)dx$</p>
<h2 id="Computation-of-Expectations"><a href="#Computation-of-Expectations" class="headerlink" title="Computation of Expectations"></a>Computation of Expectations</h2><p><strong>theorem</strong> $g$ is a Borel measurable function on $\mathbb{R}$ Then:</p>
<p>$\mathbb{E} |g(X)|&#x3D;\int_\mathbb{R} |g(x)|d\mu_X(x)$ , if this quantity is finite, then$\mathbb{E} g(X)&#x3D;\int_\mathbb{R} g(x)d\mu_X(x)$</p>
<p>PROOF</p>
<ol>
<li><p>$\mathbb{EI}_B(X)&#x3D;\mathbb{P}{X\in B}&#x3D;\mu_X(B)&#x3D;\int_\mathbb{R}\mathbb{I}_B(x)d\mu_X(x)$</p>
</li>
<li><p>nonnegative simple functions. A simple function is a finite sum of indicator functions times constants  </p>
<p> $$g(x)&#x3D;\sum\alpha_k \mathbb{I}_{B_k}(x)$$</p>
<p> so </p>
<p> $$\mathbb{E}g(X)&#x3D;\mathbb{E}\sum \alpha_k \mathbb{I}_{B_k}(X)&#x3D;\sum \alpha_k\int_R\mathbb{I}_{B_k}d\mu_X(x)&#x3D;\int_R(\sum \alpha_k\mathbb{I}_{B_k}(x))d\mu_X(x)&#x3D;\int_R g(x)d\mu_X(x)$$</p>
</li>
<li><p>when g is nonnegative Borel-measurable functions. </p>
<p> $$B_{k,n}&#x3D;{x;\frac{k}{2^n}\le g(x)\le \frac{k+1}{2^n}},k&#x3D;0,1,2,\dots,4^n-1$$</p>
<p> $$g_n(x)&#x3D;\sum \frac{k}{2^n}\mathbb{I}_{B_{k,n}}(x)$$</p>
<p> $$\mathbb{E}g(X)&#x3D;\lim \mathbb{E}g_n(X)&#x3D;\lim \int_R g_n(x)d\mu_X(x)&#x3D;\int_R g(x)d\mu_X(x)$$</p>
</li>
<li><p>when g is general Borel-measurable function.</p>
<p> $g^+(x)&#x3D;\max{g(x),0},g^-(x)\min{-g(x),0}$</p>
</li>
</ol>
<p><strong>Theorem</strong> $X$ is a random variable on a probability space $(\Omega,\mathcal{F},\mathbb{P})$, and let g be a Borel measurable function on $\mathbb{R}$. $X$ has a density $f$ </p>
<p>$\mathbb{E}|g(X)|&#x3D;\int|g(x)|f(x)dx$</p>
<p>if quantity is finite, then </p>
<p>$\mathbb{E}g(X)&#x3D;\int g(x)f(x)dx$</p>
<p>PROOF</p>
<p>simple functions</p>
<p>$$\mathbb{E}g(X)&#x3D;\mathbb{E}(\sum \alpha_k \mathbb{I}_{B_k}(X))&#x3D;\sum \alpha_k \mathbb{E}\mathbb{I}_{B_k}(X)&#x3D;\sum \alpha_k \int \mathbb{I}_{B_k}f(x)dx\\&#x3D;\int \sum \alpha<br>_k\mathbb{I}_{B_k}(x)f(x)dx&#x3D;\int g(x)f(x)dx$$</p>
<p>nonnegative Borel measurable functions</p>
<p>$$\mathbb{E}g_n(X)&#x3D;\int g_n(x)f(x)dx$$</p>
<h2 id="Change-of-Measure"><a href="#Change-of-Measure" class="headerlink" title="Change of Measure"></a>Change of Measure</h2><p>We can use a positive random variable $Z$ to change probability measures on a space $\Omega$. We need to do this when we change from the actual probability measure $\mathbb{P}$ to the risk-neutral probability measure $\widetilde{\mathbb{P}}$ in models of financial markets.</p>
<p>$$Z(\omega)\mathbb{P}(\omega)&#x3D;\widetilde{\mathbb{P}}(\omega)$$</p>
<p>to change from $\mathbb{P}$ to $\widetilde{\mathbb{P}}$, we need to reassign probabilities in $\Omega$ using $Z$ to tell us where in $\Omega$ we should revise the probability upward and where downward. </p>
<p><em>we should do this set-by-set, but not $\omega$-by-$\omega$. According to following theorem</em></p>
<p><strong>Theorem</strong> let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and let $Z$ be an almost surely nonnegative random variable with $\mathbb{E}Z&#x3D;1$. for $A\in\mathcal{F}$, define $\widetilde{\mathbb{P}}(A)&#x3D;\int_AZ(\omega)d\mathbb{P}(\omega)$. Then $\widetilde{\mathbb{P}}$ is a probability measure. If $X$ is a nonnegative random variable, then$\widetilde{\mathbb{E}}X&#x3D;\mathbb{E}[XZ]$. If $Z$ is almost surely strictly positive, we also have $\mathbb{E}Y&#x3D;\widetilde{\mathbb{E}}[\frac{Y}{Z}]$ for every nonnegative random variable $Y$.</p>
<p>PROOF</p>
<p>$\widetilde{\mathbb{P}}(\Omega)&#x3D;1$, and countably additive. </p>
<p>countably additive</p>
<p>let $A_1,A_2,A_3,\dots$ be a sequence of disjoint sets in $\mathcal{F}$, and define $B_n&#x3D;\cup_{k&#x3D;1}^n A_k$</p>
<p>we can use the monotone convergence theorem, to write</p>
<p>$$\widetilde{\mathbb{P}}(\cup _{k&#x3D;1}^\infty A_k)&#x3D;\widetilde{\mathbb{P}}(B_\infty)&#x3D;\int_\Omega \mathbb{I}_{B_\infty}(\omega)Z(\omega)d\mathbb{P}(\omega)\\&#x3D;\lim_{n\to \infty}\int_\Omega \mathbb{I}_{B_n}(\omega)Z(\omega)d\mathbb{P}(\omega)\\&#x3D;\lim_{n\to \infty}\sum\int_\Omega\mathbb{I}_{A_k}(\omega)Z(\omega)d\mathbb{P}(\omega)&#x3D;\sum_{k&#x3D;1}^\infty\widetilde{\mathbb{P}}(A_k)$$</p>
<p><strong>Definition</strong> å¦‚æœä¸¤ä¸ªæ¦‚ç‡æµ‹åº¦ä¸‹sigmaä»£æ•°ä¸­é›¶æ¦‚ç‡çš„é›†åˆç›¸åŒï¼Œé‚£ä¹ˆç§°ä¸¤ä¸ªæ¦‚ç‡æµ‹åº¦ä¸ºequivalent</p>
<p>åœ¨é‡‘èä¸­ï¼Œå®é™…æ¦‚ç‡æµ‹åº¦å’Œé£é™©ä¸­æ€§æ¦‚ç‡æµ‹åº¦å°±æ˜¯equivalentçš„ï¼Œåœ¨risk neutral worldä¸­almost workçš„hedgeåœ¨actual worldä¸­ä¸€å®šä¹Ÿalmost surely work</p>
<p><strong>Definition</strong> å¯¹äºequivalentçš„ä¸¤ä¸ªæ¦‚ç‡æµ‹åº¦ä»¥åŠå®ƒä»¬ä¹‹é—´çš„almost surely positive random variable, è¿™ä¸ªéšæœºå˜é‡$Z$ç§°ä¹‹ä¸ºRadon-Nikodym derivative of$\widetilde{\mathbb{P}}$ with respect to $\mathbb{P}$ and we write $Z&#x3D;\frac{d\widetilde{\mathbb{P}}}{d\mathbb{P}}$, $Z$ çš„å­˜åœ¨æ€§æˆä¸ºRadon-Nikodym Theorem</p>
<h1 id="Information-and-Conditioning"><a href="#Information-and-Conditioning" class="headerlink" title="Information and Conditioning"></a>Information and Conditioning</h1><h2 id="Information-and-sigma-algebras"><a href="#Information-and-sigma-algebras" class="headerlink" title="Information and $\sigma$-algebras"></a>Information and $\sigma$-algebras</h2><p>The hedge must specify what position we will take in the underlying security at each future time contingent on how the uncertainty between the present time and that future time is resolved.</p>
<p><strong>resolve sets by information</strong></p>
<p>$\Omega$ is the set of 8 possible outcomes of 3 coin tosses,</p>
<p>$A_H&#x3D;{HHH,HHT,HTH,HTT},A_T&#x3D;{THH,THT,TTH,TTT}$</p>
<p>if we are told the first coin toss only. the four sets that are resolved by the first coin toss form the $\sigma$-algebra</p>
<p>$\mathcal{F}_1&#x3D;{\phi,\Omega,A_H,A_T}$</p>
<p>this $\sigma$-algebra contains the information learned by observing the first coin toss.</p>
<p>å¦‚æœæŸå‡ ä¸ªé›†åˆæ˜¯resolvedï¼Œé‚£ä¹ˆä»–ä»¬çš„å¹¶ï¼Œä»¥åŠå„è‡ªçš„è¡¥ä¹Ÿæ˜¯resolvedï¼Œç¬¦åˆ$\sigma$-algebra</p>
<p><strong>Definition</strong> let $\Omega$ be a nonempty set. Let $T$ be a fixed positive number and assume that for each $t\in [0,T]$ there is a $\sigma-algebra$  $\mathcal{F}(t)$. Assume further that if $s\le t$, then every set in $\mathcal{F}(s)$ is also in $\mathcal{F}(t)$. Then we call the collection of $\sigma-algebra$ $\mathcal{F}(t)$ a filtration.</p>
<p>At time $t$ we can know whether the true $\omega$  lies in a set in $\mathcal{F}(t)$</p>
<p>Let $\Omega&#x3D;C_0[0,T]$, and assign probability to the sets in $\mathcal{F}&#x3D;\mathcal{F}(T)$, then the paths $\omega\in C_0[0,T]$ will be the paths of the <strong>Brownian motion</strong>.</p>
<p><strong>Definition</strong> Let $X$ be a random variable defined on a nonempty sample space $\Omega$. The $\sigma-algebra$ generated by $X$, denoted $\sigma(X)$, is the collection of all subsets of $\Omega$ of the form ${X\in B}$, $B$<br>  <strong>ranges over</strong> the Borel subsets of $\mathbb{R}$</p>
<p>è¿™é‡Œçš„åŒºåˆ«å’Œç®€å¹¶æœ‰ç‚¹åƒï¼Œéšæœºå˜é‡æ˜¯æœ¬å¾å€¼ï¼ŒFiltrationæ˜¯é‡å­æ€</p>
<p><strong>Definition</strong>  Let $X$ be a random variable defined on a nonempty sample space $\Omega$. Let  $\mathcal{G}$ be a $\sigma-algebra$ of subsets of $\Omega$. If every set in $\sigma(X)$ is also in $\mathcal{G}$, we say that $X$ is $\mathcal{G}-measurable$</p>
<p>æ„å‘³ç€$\mathcal{G}$ ä¸­çš„ä¿¡æ¯è¶³å¤Ÿå†³å®š$X$ çš„å€¼ã€‚å¯¹äºborelå¯æµ‹å‡½æ•°$f$ï¼Œ$f(X)$ä¹Ÿæ˜¯$\mathcal{G}-measurable$çš„ã€‚å¯¹äºå¤šå…ƒå‡½æ•°åŒæ ·æˆç«‹</p>
<p><strong>Definition</strong> Let  $\Omega$ be a nonempty sample space equipped with a filtration $\mathcal{F}(t)$. Let </p>
<p>$X(t)$ be a collection of random variables indexed by $t\in [0,T]$, We say this collection of random variables is an adapted stochastic process if, for each $t$, the random variable $X(t)$ is $\mathcal{F}(t)-measurable$ </p>
<h2 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h2><p>å¯¹äºä¸€ä¸ªéšæœºå˜é‡å’Œä¸€ä¸ª$\sigma-algebra$ $\mathcal{G}$, measurable å’Œ independent æ˜¯ä¸¤ç§æç«¯ï¼Œä½†å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œ$\mathcal{G}$ä¸­ä½†ä¿¡æ¯å¯ä»¥ä¼°è®¡$X$ï¼Œ ä½†ä¸è¶³ä»¥ç¡®å®š$X$çš„å€¼ã€‚</p>
<p>Independenceä¼šå—åˆ°æ¦‚ç‡åº¦é‡çš„å½±å“ï¼Œä½†measurabilityä¸ä¼šã€‚</p>
<p>probability space $(\Omega,\mathcal{F},\mathbb{P})$,$\mathcal{G}\in \mathcal{F}, \mathcal{H}\in \mathcal{F}$, $\Omega$ ä¸­çš„ä¸¤ä¸ª$\sigma-algebra$$A$å’Œ$B$ç‹¬ç«‹å¦‚æœ$\mathbb{P}(A\cap B)&#x3D;\mathbb{P}(A)\cdot\mathbb{P}(B)$, for all $A\in \mathcal{G},B\in \mathcal{H}$</p>
<p>ç‹¬ç«‹çš„å®šä¹‰å¯ä»¥æ‹“å±•åˆ°éšæœºå˜é‡($\sigma-algebra$ they generate are independent)ä¹‹é—´ï¼Œéšæœºå˜é‡ä¸$\sigma-algebra$ä¹‹é—´ã€‚</p>
<p><strong>Definition</strong>  $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space and let $\mathcal{G}_1,\mathcal{G}_2,\mathcal{G}_3\dots$ be a sequence of $sub-\sigma-algebra$ of $\mathcal{F}$. For a fixed positive integer n, we say that the n $\sigma-algebra$s $\mathcal{G}_1,\mathcal{G}_2,\mathcal{G}_3\dots\mathcal{G}_n$ are independent if$\mathbb{P}(A_1\cap A_2\cap \dots\cap A_n)&#x3D;\mathbb{P}<br>(A_1)\mathbb{P}<br>(A_2)\dots \mathbb{P}<br>(A_n)$ for all $A_i\in \mathcal{G}_i$</p>
<p>è¿™ä¸ªåŒæ ·ä¹Ÿå¯ä»¥æ‹“å±•åˆ°éšæœºå˜é‡ä¹‹é—´ä»¥åŠéšæœºå˜é‡ä¸$\sigma-algebra$ä¹‹é—´ã€‚</p>
<p><strong>Theorem</strong> Let $X$ and $Y$ be independent random variables, and let $f$  and $g$ be Borel measurable functions on $\mathbb{R}$. Then$f(X)$ and $g(Y)$ are independent random variables.</p>
<p>PROOF</p>
<p>Let $A$ be in the $\sigma-algebra$ generated by $f(X)$. This $\sigma-algebra$ is a $sub-\sigma -algebra$ of $\sigma(X)$. </p>
<p>è¿™å¾ˆè‡ªç„¶ï¼Œevery set in $A$ is of the form${\omega \in \Omega;f(X(\omega))\in C}\text{,where }C\text{ is a Borel subset of }\mathbb{R}$. åªéœ€è¦å®šä¹‰$D&#x3D;{x\in \mathbb{R};f(x)\in C}$. Then we know$A\in \sigma(X)$. Let $B$ be the $\sigma-algebra$ gnenerated by $g(Y)$, then $B\in \sigma(Y)$. ç”±äº$X,Y$ç‹¬ç«‹ï¼Œæ‰€ä»¥$A,B$ç‹¬ç«‹ã€‚</p>
<p><strong>Definition</strong> Let$X$ and $Y$ be random variables. The pair of random variables$(X,Y)$ takes values in the plane$\mathbb{R}^2$, and the joint distribution measure of $(X,Y)$ is given by$\mu_{X,Y}(C)&#x3D;\mathbb{P}{(X,Y)\in C}$ for all Borel sets $C\in \mathbb{R}^2$</p>
<p>åŒæ ·æ»¡è¶³æ¦‚ç‡åº¦é‡çš„ä¸¤ä¸ªæ¡ä»¶ï¼šå…¨ç©ºé—´æµ‹åº¦ä¸º1ï¼Œä»¥åŠcountable additivity preperty</p>
<p><strong>Theorem</strong> Let $X$ and $Y$ be random variables. The following conditions and equivalent.</p>
<ol>
<li><p>$X$ and  $Y$ are independent</p>
</li>
<li><p>the joint distribution measure factors:</p>
<p> $\mu_{X,Y}(A\times B)&#x3D;\mu_X(A)\mu_Y(B)$ for all Borel subsets$A\in \mathbb{R},B\in\mathbb{R}$</p>
</li>
<li><p>the joint cumulative distribution function factors</p>
<p> $F_{X,Y}(a,b)&#x3D;F_X(a)F_Y(b)$ for all $a\in \mathbb{R},b\in \mathbb{R}$</p>
</li>
<li><p>the joint moment-generating function factor:(çŸ©æ¯å‡½æ•°ï¼‰</p>
<p> $\mathbb{E}e^{uX+vY}&#x3D;\mathbb{E}e^{uX}\cdot\mathbb{E}e^{vY}$</p>
</li>
<li><p>the joint density factors(if there is one)</p>
<p> $f_{X,Y}(x,y)&#x3D;f_X(x)f_Y(y)$ for almost every$x\in\mathbb{R},y\in\mathbb{R}$</p>
</li>
</ol>
<p>conditions above imply but are not equivalent to:</p>
<ol>
<li>$\mathbb{E}[XY]&#x3D;\mathbb{E}X\cdot\mathbb{E}Y$ provided $\mathbb{E}|XY|&lt;\infty$</li>
</ol>
<p><strong>Definition</strong> Let $X$ be a random variable whose expected value is defined. The variance of $X$, denoted $Var(X)$ is $Var(X)&#x3D;\mathbb{E}[(X-\mathbb{E}X)^2]&#x3D;\mathbb{E}[X^2]-(\mathbb{E}X)^2$</p>
<p>covariance of $X$ and $Y$ is $Cov(X,Y)&#x3D;\mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)]&#x3D;\mathbb{E}[XY]-\mathbb{E}X\cdot\mathbb{E}Y$</p>
<p>correlation coefficient of $X$and $Y$ is $\rho(X,Y)&#x3D;\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$</p>
<p>if $\rho(X,Y)&#x3D;0$, we say that $X$ and $Y$ are uncorrelated</p>
<h2 id="General-Conditional-Expectations"><a href="#General-Conditional-Expectations" class="headerlink" title="General Conditional Expectations"></a>General Conditional Expectations</h2><p><strong>Definition</strong> Let$(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\mathcal{F}$, and let $X$ be a random variable that is either nonnegative or integrable. The conditional expectation of $X$ given $\mathcal{G}$, denoted$\mathbb{E}[X|\mathcal{G}]$, is any random variable that satisfies</p>
<ol>
<li><strong>Measurability $\mathbb{E}[X|\mathcal{G}]$</strong>  is $\mathcal{G}$ measurable</li>
<li><strong>Partial averaging $\int_A\mathbb{E}[X|\mathcal{G}(\omega)]&#x3D;\int_AX(\omega)d\mathbb{P}(\omega)$</strong> for all $A\in \mathcal{G}$</li>
</ol>
<p>If $\mathcal{G}$ is generated by some other random variable $W$, then we can write$\mathbb{E}[X|W]$</p>
<p>(1) means although the estimate of $X$based on the information in $\mathcal{G}$ is itself a random variable, the value of the estimate $\mathbb{E}[X|\mathcal{G}]$ can be determined from the information in $\mathcal{G}$</p>
<p>å­˜åœ¨æ€§å¯ä»¥ç”±Radon-Nikodym Theoremè¯æ˜</p>
<p>å”¯ä¸€æ€§çš„è¯æ˜å¦‚ä¸‹ï¼šIf $Y$ and $Z$ both satisfy conditions, then their difference $Y-Z$ is as well, and thus the set $A&#x3D;{Y-Z&gt;0}$ is in $\mathcal{G}$. æ ¹æ®(2),$\int_AY(\omega)d\mathbb{P}(\omega)&#x3D;\int_AX(\omega)d\mathbb{P}(\omega)&#x3D;\int_AZ(\omega)d\mathbb{P}(\omega)$</p>
<p>Hence $Y&#x3D;Z$ almost surely.</p>
<p><strong>Theorem</strong> Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\mathcal{F}$</p>
<ol>
<li><p>linearity of conditional expectations<br>$\mathbb{E}[c_1X+c_2Y|\mathcal{G}]&#x3D;c_1\mathbb{E}[X|\mathcal{G}]+c_2\mathbb{E}[Y|\mathcal{G}]$</p>
</li>
<li><p>taking out what is known. </p>
<p> If $X$ is $\mathcal{G}$-measurable</p>
<p> $\mathbb{E}[XY|\mathcal{G}]&#x3D;X\mathbb{E}[Y|\mathcal{G}]$</p>
</li>
<li><p>iterated conditioning.</p>
<p> If $\mathcal{H}$ is a sub-$\sigma$-algebra of $\mathcal{G}$ and $X$ is an integrable random variable, then</p>
<p> $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]&#x3D;\mathbb{E}[X|\mathcal{H}]$</p>
</li>
<li><p>independence</p>
<p> If $X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X|\mathcal{G}]&#x3D;\mathbb{E}X$</p>
</li>
<li><p>conditional Jensenâ€™s inequality</p>
<p> $\mathbb{E}[\phi(X)|\mathcal{G}]\ge\phi(\mathbb{E}[X|\mathcal{G}])$</p>
</li>
</ol>
<p><strong>Lemma</strong> Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Suppose the random variables $X_1,\dots,X_K$ are $\mathcal{G}$-measurable and the random variables$Y_1,\dots,Y_L$ are independent of $\mathcal{G}$. Let $f(x_1,\dots,x_K,y_1,\dots,y_L)$ be a function of the dummy variables, and define: </p>
<p>$g(x_1,\dots,x_K)&#x3D;\mathbb{E}f(x1,\dots,x_K,Y_1,\dots,Y_L)$</p>
<p>This means holding $X_i$ constant and integrate out $Y_i$.</p>
<p>Then,$\mathbb{E}[f(X_1,\dots,X_K,Y_1,\dots,Y_L)|\mathcal{G}]&#x3D;g(X_1,\dots,X_K)$</p>
<p><strong>Definition</strong> Let $(\Omega,\mathcal{F},\mathbb{P})$  be a probability space, let $T$ be a fixed positive number, and let $\mathcal{F}(t)$, $0\le t\le T$, be a filtration of $sub-\sigma-algebra$ of $\mathcal{F}$. Consider an adapted stochastic process $M(t)$:</p>
<ol>
<li>If $\mathbb{E}[M(t)|\mathcal{F}(s)]&#x3D;M(s)$ for all $0\le s\le t \le T$, this process is a <em><strong>martingale</strong></em>. It has no tendency to rise or fall.</li>
<li>If $\mathbb{E}[M(t)|\mathcal{F}(s)]\le M(s)$ for all $0\le s\le t \le T$, this process is a <em><strong>supermartingale</strong></em>. It has no tendency to rise, it may have a tendency to fall.</li>
<li>If $\mathbb{E}[M(t)|\mathcal{F}(s)]\ge M(s)$ for all $0\le s\le t \le T$, this process is a <em><strong>submartingale</strong></em>. It has no tendency to fall, it may have a tendency to rise.</li>
</ol>
<p><strong>Definition</strong> Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, let $T$ be a fixed positive number, and let  $\mathcal{F}(t)$, $0\le t\le T$, be a filtration of $sub-\sigma-algebra$ of $\mathcal{F}$. Consider an adapted stochastic process $X(t)$. Assume that for all $0\le s\le t\le T$ and for every nonnegative, Borel-measuable function $f$, there is another Borel-measuable function $g$ such that:</p>
<p>$\mathbb{E}[f(X(t))|\mathcal{F}(s)]&#x3D;g(X(s))$</p>
<p>Then we say that the X is a Markov process</p>
<h1 id="Brownian-Motion"><a href="#Brownian-Motion" class="headerlink" title="Brownian Motion"></a>Brownian Motion</h1><h2 id="Scaled-Random-Walks"><a href="#Scaled-Random-Walks" class="headerlink" title="Scaled Random Walks"></a>Scaled Random Walks</h2><h3 id="Symmetric-Random-Walk"><a href="#Symmetric-Random-Walk" class="headerlink" title="Symmetric Random Walk"></a>Symmetric Random Walk</h3><p>To build a <strong>symmetric random walk</strong> we repeatedly toss a fair coin.</p>
<p>$X_j&#x3D;\begin{cases}1 \text{ if }\omega_j&#x3D;H\\-1\text{ if }\omega_j&#x3D;T\end{cases}$</p>
<p>$M_0&#x3D;0$, $M_k&#x3D;\sum_{j&#x3D;1}^kX_j$</p>
<p>the process $M_k$ is a <em>symmetric random walk.</em></p>
<h3 id="Increments-of-the-Symmetric-Random-Walk"><a href="#Increments-of-the-Symmetric-Random-Walk" class="headerlink" title="Increments of the Symmetric Random Walk"></a>Increments of the Symmetric Random Walk</h3><p>A random walk has independent increments(ç‹¬ç«‹å¢é‡æ€§ï¼‰</p>
<p>if we choose nonnegative integers$0&#x3D;k_0&lt;k_1&lt;\cdots&lt;k_m$ , the random variables $M_{k_i}-M_{k_{i-1}}$ are independent.</p>
<p>Each of these random variables $M_{k_i}-M_{k_{i-1}}&#x3D;\sum_{j&#x3D;k_{i+1}}^{k_{i+1}}X_j$ is called an increment of the random walk. </p>
<p>Each increment has expected value 0 and variance $k_{i+1}-k_i$ . The variance is obvious because $X_j$ are independent.</p>
<h3 id="Martingale-Property-for-the-Symmetric-Random-Walk"><a href="#Martingale-Property-for-the-Symmetric-Random-Walk" class="headerlink" title="Martingale Property for the Symmetric Random Walk"></a>Martingale Property for the Symmetric Random Walk</h3><p>We choose nonnegative integers $k&lt;l$ and compute:</p>
<p>$\mathbb{E}[M_l|\mathcal{F}_k]&#x3D;\mathbb{E}[(M_l-M_k)+M_k|\mathcal{F}_k]&#x3D;\mathbb{E}[M_l-M_k|\mathcal{F}_k]+\mathbb{E}[M_k|\mathcal{F}_k]&#x3D;M_k$</p>
<h3 id="Quadratic-Variation-of-the-Symmetric-Random-Walk"><a href="#Quadratic-Variation-of-the-Symmetric-Random-Walk" class="headerlink" title="Quadratic Variation of the Symmetric Random Walk"></a>Quadratic Variation of the Symmetric Random Walk</h3><p>the quadratic variation up to time $k$ is defined to be  $[M,M]_k&#x3D;\sum_{j&#x3D;1}^k(M_j-M_{j-1})^2&#x3D;k$</p>
<p>This is computed path-by-path. taking all the one step increments $M_j-M_{j-1}$ along that path, squaring these increments, and then summing them.</p>
<p>Note that $[M,M]_k$ is the same as $Var(M_k)$, but the computations of these two quantities are quite different. $Var$ is computed by taking an average over all paths, taking their probability into account. If the random walk is not symmetric, the probability distribution would affect $Var$. On the contrary, the probability up and down do not affect the quadratic variation computation.</p>
<h3 id="Scaled-Symmetric-Random-Walk"><a href="#Scaled-Symmetric-Random-Walk" class="headerlink" title="Scaled Symmetric Random Walk"></a>Scaled Symmetric Random Walk</h3><p>We fix a positive integer $n$ and define the <strong>scaled symmetric random walk</strong> $W^{(n)}(t)&#x3D;\frac{1}{\sqrt{n}}M_{nt}$. If $nt$ is not an integer, we define $W^{(n)}(t)$ by linear interpolation between its values at the nearest points $s$ and $u$ to the left and right of $t$ for which $ns$ and $nu$ are integers. </p>
<p>A Brownian motion is a scaled symmetric random walk with $n\to \infty$</p>
<p>$\mathbb{E}(W^{(n)}(t)-W^{(n)}(s))&#x3D;0,Var(W^{(n)}(t)-W^{(n)}(s))&#x3D;t-s$</p>
<p>Let $0\le s\le t$ be given, and decompose $W^{(n)}(t)$ as: $W^{(n)}(t)&#x3D;(W^{(n)}(t)-W^{(n)}(s))+W^{(n)}(s)$ </p>
<p>$(W^{(n)}(t)-W^{(n)}(s))$ is independent of $\mathcal{F}(s)$, the $\sigma-algebra$ of information available at time s, and $W^{(n)}(s)$ is $\mathcal{F}(s)$ measurable. So $\mathbb{E}[W^{(n)}(t)|\mathcal{F}(s)]&#x3D;W^{(n)}(s)$</p>
<p>The <strong>quadratic variation</strong> of the scaled random walk: </p>
<p>$[W^{(n)},W^{(n)}](t)&#x3D;\sum_{j&#x3D;1}^{nt}[W^{(n)}(\frac{j}{n})-W^{(n)}(\frac{j-1}{n})]^2&#x3D;\sum_{j&#x3D;1}^{nt}[W^{(n)}[\frac{1}{\sqrt{n}}X_j]^2&#x3D;\sum_{j&#x3D;1}^{nt}\frac{1}{n}&#x3D;t$</p>
<h3 id="Limiting-Distribution-of-the-Scaled-Random-Walk"><a href="#Limiting-Distribution-of-the-Scaled-Random-Walk" class="headerlink" title="Limiting Distribution of the Scaled Random Walk"></a>Limiting Distribution of the Scaled Random Walk</h3><p><strong>Theorem: Central limit</strong> Fix $t\ge 0$. As $n\to \infty$, the distribution of the scaled random walk $W^{(n)}(t)$ evaluated at time $t$ converges to the normal distribution with mean zero and variance $t$.</p>
<p>PROOF</p>
<p>For the normal density $f(s)&#x3D;\frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}$, the moment-generating function is </p>
<p>$\phi(u)&#x3D;\frac{1}{\sqrt{2\pi t}}\int_{-\infty}^\infty \exp{ux-\frac{x^2}{2t}}dx&#x3D;e^{\frac{1}{2}u^2t}$</p>
<p>If $t$ is such that $nt$ is an integer, then the moment-generating function for $W^{(n)}(t)$ is:</p>
<p>$\phi_n(u)&#x3D;\mathbb{E}e^{uW^{(n)}(t)}&#x3D;\mathbb{E}\exp{\frac{u}{\sqrt{n}}M_{nt}}&#x3D;\mathbb{E}\exp{\frac{u}{\sqrt{n}}\sum_{j&#x3D;1}^{nt}X_j}&#x3D;\mathbb{E}\prod_{j&#x3D;1}^{nt}\exp{\frac{u}{\sqrt{n}}X_j}\\&#x3D;\prod_{j&#x3D;1}^{nt}\mathbb{E}\exp{\frac{u}{\sqrt{n}}X_j}&#x3D;\prod_{j&#x3D;1}^{nt}(\frac{e^{\frac{u}{\sqrt{n}}}+e^{\frac{-u}{\sqrt{n}}}}{2})&#x3D;(\frac{e^{\frac{u}{\sqrt{n}}}+e^{\frac{-u}{\sqrt{n}}}}{2})^{nt}$</p>
<p>when $n\to \infty$ , $\phi(u)&#x3D;e^{\frac{1}{2}u^2t}$</p>
<h3 id="Log-Normal-Distribution-as-the-Limit-of-the-Binomial-Model"><a href="#Log-Normal-Distribution-as-the-Limit-of-the-Binomial-Model" class="headerlink" title="Log-Normal Distribution as the Limit of the Binomial Model"></a>Log-Normal Distribution as the Limit of the Binomial Model</h3><p>We build a model for a stock price on the time interval from $0$ to $t$ by choosing an integer $n$  and constructing a binomial model for the stock price that takes $n$ steps per unit time. Up factor $u_n&#x3D;1+\frac{\sigma}{\sqrt{n}}$, down factor $u_d&#x3D;1-\frac{\sigma}{\sqrt{n}}$. $r&#x3D;0.$</p>
<p>The risk-neutral probabilities are then:</p>
<p>$\tilde{p}&#x3D;\frac{1+r-d_n}{u_n-d_n}&#x3D;\frac{1}{2}$, $\tilde{q}&#x3D;\frac{u_n-1-r}{u_n-d_n}&#x3D;\frac{\sigma&#x2F;\sqrt{n}}{2\sigma&#x2F;\sqrt{n}}&#x3D;\frac{1}{2}$</p>
<p>Random walk $M_{nt}&#x3D;H_{nt}-T_{nt}$, while $nt&#x3D;H_{nt}+T_{nt}$</p>
<p>the stock price at time $t$ is $S_n(t)&#x3D;S(0)u_n^{H_{nt}}d_n^{T_{nt}}&#x3D;S(0)(1+\frac{\sigma}{\sqrt{n}})^{\frac{1}{2}(nt+M_{nt})}(1+\frac{\sigma}{\sqrt{n}})^{\frac{1}{2}(nt+M_{nt})}(1-\frac{\sigma}{\sqrt{n}})^{\frac{1}{2}(nt-M_{nt})}$</p>
<p>we need to identify the distribution of this random variable as $n\to \infty$</p>
<p><strong>Theorem</strong> As $n\to \infty$ , the distribution of $S_n(t)$ converges to the distribution of</p>
<p>$S(t)&#x3D;S(0)\exp{\sigma W(t)-\frac{1}{2}\sigma^2t}$ </p>
<p>$W(t)$ is a normal random variable with mean zero and variance $t$</p>
<p>PROOF:</p>
<p>$\log S_n(t)\\&#x3D;\log S(0)+\frac{1}{2}(nt+M_{nt})\log (1+\frac{\sigma}{\sqrt{n}})+\frac{1}{2}(nt-M_{nt})\log (1-\frac{\sigma}{\sqrt{n}})\\&#x3D;\log S(0)+\frac{1}{2}(nt+M_{nt})(\frac{\sigma}{\sqrt{n}}-\frac{\sigma^2}{2n}+O(n^{-3&#x2F;2}))+\frac{1}{2}(nt-M_{nt})(\frac{\sigma}{-\sqrt{n}}-\frac{\sigma^2}{2n}+O(n^{-3&#x2F;2}))\\&#x3D;\log S(0)+nt(-\frac{\sigma^2}{2n}+O(n^{-3&#x2F;2}))+M_{nt}(\frac{\sigma}{\sqrt{n}}+O(n^{-3&#x2F;2}))&#x3D;\log S(0)-\frac{1}{2}\sigma^2t+\sigma W^{(n)}(t)$</p>
<p>By the <strong>Central Limit Theorem,</strong> we know that $\frac{1}{\sqrt{n}}M_{nt}&#x3D;W^{(n)}(t)\to W(t), \text{when }n\to \infty$</p>
<h2 id="Brownian-Motion-1"><a href="#Brownian-Motion-1" class="headerlink" title="Brownian Motion"></a>Brownian Motion</h2><h3 id="Definition-of-Brownian-Motion"><a href="#Definition-of-Brownian-Motion" class="headerlink" title="Definition of Brownian Motion"></a>Definition of Brownian Motion</h3><p><strong>Definition</strong> Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. For each $\omega\in\Omega$, suppose there is a continuous function $W(t)$ of $t\ge 0$ that satisfies $W(0)&#x3D;0$ that depends on $\omega$. Then $W(t),t\ge 0$ is a Brownian motion if for all $t_i$ the increments $W(t_i)-W(t_{i-1})$ are independent and each of these increments is normally distributed with $\mathbb{E}[W(t_{i+1})-W(t_i)]&#x3D;0,Var[W(t_{i+1})-W(t_i)]&#x3D;t_{i+1}-t_i$</p>
<p>Difference between BM and a scaled random walk: the scaled random walk has a natural time step and is linear between these time steps, whereas the BM has no linear pieces.</p>
<h3 id="Distribution-of-Brownian-Motion"><a href="#Distribution-of-Brownian-Motion" class="headerlink" title="Distribution of Brownian Motion"></a>Distribution of Brownian Motion</h3><p>For any two times, the covariance of $W(s)$ and $W(t)$  for $s\le t$ is</p>
<p>$\mathbb{E}[W(s)W(t)]&#x3D;\mathbb{E}[W(s)(W(t)-W(s))+W^2(s)]&#x3D;\mathbb{E}[W(s)]\mathbb{E}[(W(t)-W(s))]+\mathbb{E}[W^2(s)]&#x3D;s$</p>
<p>We compute the moment-generating function of the random vector$(W(t_1),W(t_2),\dots,W(t_m))$</p>
<p>$\phi&#x3D;\mathbb{E}\exp{u_mW(t_m)+\dots+u_1W(t_1)}&#x3D;\mathbb{E}\exp{u_m(W(t_m)-W(t_{m-1}))+\dots+(u_1+u_2+\dots+u_m)W(t_1)}&#x3D;\mathbb{E}\exp{u_m(W(t_m)-W(t_{m-1}))}\cdots \mathbb{E}\exp{(u_1+u_2+\cdots+u_m)W(t_1)}&#x3D;\exp{\frac{1}{2}u_m^2(t_m-t_{m-1})}\cdots\exp{(u_1+u_2+\cdots+u_m)^2t_1}$</p>
<p>The distribution of the Brownian increments can be specified by specifying the joint density or the joint moment-generating function of the random variables</p>
<p><strong>Theorem</strong> Let$(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. For each $\omega\in\Omega$, suppose there is a continuous function $W(t)$ of $t\ge 0$ that satisfies $W(0)&#x3D;0$ and that depends on $\omega$. </p>
<ol>
<li><p>For all $0&lt; t_0&lt; t_1\cdots &lt;t_m$, the increments are independent and each of these increments is normally distributed with mean and variance given by$\mathbb{E}[W(t_{i+1})-W(t_i)]&#x3D;0,Var[W(t_{i+1})-W(t_i)]&#x3D;t_{i+1}-t_i$</p>
</li>
<li><p>For all $0&lt; t_0&lt; t_1\cdots &lt;t_m$, the random variables $W(t_i)$ are jointly normally distributed with means equal to zero and covariance matrix</p>
<p> $\begin{pmatrix}Â Â Â  t_{1} &amp;t_1 \cdots &amp; t_{1} \\Â Â t_1&amp;t_2\cdots &amp;t_2\\Â  \vdots &amp; \ddots &amp; \vdots \\Â Â Â  t_{1} &amp;t_2 \cdots &amp; t_{m}Â Â \end{pmatrix}$Â </p>
</li>
<li><p>the random variables have the joint moment-generating function mentioned before</p>
</li>
</ol>
<h3 id="Filtration-for-Brownian-Motion"><a href="#Filtration-for-Brownian-Motion" class="headerlink" title="Filtration for Brownian Motion"></a>Filtration for Brownian Motion</h3><p><strong>Definition</strong> Let$(\Omega,\mathcal{F},\mathbb{P})$ be a probability space on which is defined a Brownian motion $W(t),t\ge 0$. A filtration for the Brownian motion is a collection of $\sigma$-algebra $\mathcal{F}(t),t\ge 0$, satisfying:</p>
<ol>
<li><strong>Information accumulates</strong></li>
<li><strong>Adaptivity: $W(t)$</strong> is <strong>$\mathcal{F}(t)$</strong>-measurable</li>
<li><strong>Independence of future increments</strong></li>
</ol>
<p>$\Delta(t),t\ge0$, be a stochastic process. $\Delta(t)$ is adapted to the filtration $\mathcal{F}(t)$ if for each $t\ge 0$ the random variable $\Delta(t)$ if $\mathcal{F}(t)$-measurable.</p>
<p>There are two possibilities for the filtration $\mathcal{F}(t)$ for a Brownian motion. </p>
<ol>
<li>to let $\mathcal{F}(t)$ contain only the info obtained by observing the BM itself up to time $t$. </li>
<li>to include in $\mathcal{F}(t)$ info obtained by observing the BM and one or more other processes. But if the info in $\mathcal{F}(t)$ includes observations of processes other than the BM $W$, this additional info is not allowed to give clues about the future increments because of property iii</li>
</ol>
<h3 id="Martingale-Property-for-Brownian-Motion"><a href="#Martingale-Property-for-Brownian-Motion" class="headerlink" title="Martingale Property for Brownian Motion"></a>Martingale Property for Brownian Motion</h3><p><strong>Theorem</strong> Brownian motion is a martingale</p>
<h2 id="Quadratic-Variation"><a href="#Quadratic-Variation" class="headerlink" title="Quadratic Variation"></a>Quadratic Variation</h2><p>For BM, there is no natural step size. If we are given $T&gt;0$, we could simply choose a step size, say $\frac{T}{n}$ for some large $n$, and compute the quadratic variation up to time $T$ with this step size:</p>
<p>$\sum_{j&#x3D;0}^{n-1}[W(\frac{(j+1)T}{n})-W(\frac{jT}{n})]^2$</p>
<p>The variation of paths of BM is not zero, which makes stochastic calculus different from ordinary calculus</p>
<h3 id="First-Order-Variation"><a href="#First-Order-Variation" class="headerlink" title="First-Order Variation"></a>First-Order Variation</h3><p>$FV_T(f)&#x3D;|f(t_1)-f(0)|+|f(t_2)-f(t_1)|+\dots+|f(T)-f(t_2)|&#x3D;\int_0^{t_1}fâ€™(t)dt+\dots+\int_{t_2}^Tfâ€™(t)dt&#x3D;\int_0^T|fâ€™(t)|dt$</p>
<p>We first choose a partition $\Pi&#x3D;{t_0,t_1,\dots,t_n}$ of $[0,T]$, which is a set of times. These will serve to determine the step size. The maximum step size of the partition will be denoted $\Vert\Pi\Vert&#x3D;\max_{j&#x3D;0,\dots,n-1}(t_{j+1}-t_j)$. We then define:</p>
<p>$FV_T(f)&#x3D;\lim_{\Vert\Pi\Vert\to0}\sum_{j&#x3D;0}^{n-1}\vert f(t_{j+1})-f(t_j)\vert$</p>
<p>ç”¨ä¸­å€¼å®šç†å¯ä»¥è¯æ˜ä¸ç§¯åˆ†ç›¸ç­‰</p>
<h3 id="Quadratic-Variation-1"><a href="#Quadratic-Variation-1" class="headerlink" title="Quadratic Variation"></a>Quadratic Variation</h3><p><strong>Definition</strong> Let $f(t)$  be a function defined for $0\le t\le T$. The quadratic variation to $f$ up to time $T$ is </p>
<p>$[f,f](T)&#x3D;\lim_{\Vert\Pi\Vert\to0}\sum_{j&#x3D;0}^{n-1}\vert f(t_{j+1})-f(t_j)\vert$</p>
<p>if $f$ is derivative, then $<a href="T">f,f</a>&#x3D;0$</p>
<p>if $\int_0^T\vert fâ€™(t^*_j)\vert ^2dt$ is infinite, then $[f,f](T)$ lead to a $0\cdot\infty$ situation, which can be anything  between $0$ and $\infty$</p>
<p><strong>Theorem</strong> Let $W$ be a Brownian motion. Then $[W,W](T)&#x3D;T$ for all $T\ge 0$ almost surely.</p>
<p>PROOF</p>
<p>Define the <em>sample quadratic variation</em> corresponding to the partition of $[0,T]$,  $\Pi &#x3D;{t_0,t_1,\dots,t_n}$ to be</p>
<p> $Q_\Pi&#x3D;\sum_{j&#x3D;0}^{n-1}(W(t_{j+1})-W(t_j))^2$</p>
<p>We can show that $Q_\Pi$ converges to $T$ as $\Vert\Pi\Vert\to0$</p>
<p>$\mathbb{E}[(W(t_{j+1})-W(t_j))^2]&#x3D;Var[W(t_{j+1})-W(t_j)]&#x3D;t_{j+1}-t_j$ implies:</p>
<p>$\mathbb{E}Q_\Pi&#x3D;\sum\mathbb{E}[(W(t_{j+1})-W(t_j))^2]&#x3D;\sum(t_{j+1}-t_j)&#x3D;T$</p>
<p>$Var[(W(t_{j+1})-W(t_j))^2]&#x3D;\mathbb{E}[(W(t_{j+1})-W(t_j))^4]-2(t_{j+1}-t_j)\mathbb{E}[(W(t_{j+1})-W(t_j))^2]+(t_{j+1}-t_j)^2$</p>
<p>$\mathbb{E}[(W(t_{j+1})-W(t_j))^4]&#x3D;3(t_{j+1}-t_j)^2$,$\mathbb{E}[(W(t_{j+1})-W(t_j))^2]&#x3D;t_{j+1}-t_j$</p>
<p>PROOF</p>
<p>By Itoâ€™s formula, we have</p>
<p>$ğ‘Š^4(t)&#x3D;4\int^ğ‘¡_0ğ‘Š^3_sğ‘‘ğ‘Š_s+6\int^ğ‘¡_0ğ‘Š^2_sds$</p>
<p>$M(t):&#x3D;\int_0^tW_s^3dW_s&#x3D;0$ is a martingale and $\mathbb{E}M(t)&#x3D;\mathbb{E}M(0)&#x3D;0$</p>
<p>$\mathbb{E}W^4(t)&#x3D;6\int_0^t\mathbb{E}[W_s^2]ds&#x3D;6\int_0^tsds&#x3D;3t^2$</p>
<p>so $Var[(W(t_{j+1}-W(t_j))^4]&#x3D;2(t_{j+1}-t_j)^2$</p>
<p>and $Var(Q_\Pi)&#x3D;\sum 2(t_{j+1}-t_j)^2\le2\Vert\Pi\Vert T$</p>
<p>In particular,  $\lim_{\Vert\Pi\Vert\to 0}Var(Q_\Pi)&#x3D;0,lim_{\Vert\Pi\Vert\to0}Q_\Pi&#x3D;\mathbb{E}Q_\Pi&#x3D;T$</p>
<p>$(W(t_{j+1})-W(t_j))^2\approx t_{j+1}-t_j$$(W(t_{j+1})-W(t_j))^2&#x3D;t_{j+1}-t_j$ when $t_{j+1}-t_j$ is very small</p>
<p>$dW(t)dW(t)&#x3D;dt$ is the informal write</p>
<p>$dWdt&#x3D;0,dtdt&#x3D;0$</p>
<h3 id="Volatility-of-Geometric-Brownian-Motion"><a href="#Volatility-of-Geometric-Brownian-Motion" class="headerlink" title="Volatility of Geometric Brownian Motion"></a>Volatility of Geometric Brownian Motion</h3><p>geometric Brownian motion: $S(t)&#x3D;S(0)\exp{\sigma W(t)+(\alpha-\frac{1}{2}\sigma^2)t}$</p>
<p>realized volatility: the sum of the squares of the log returns</p>
<p>for $T_1&#x3D;t_0&lt;t_1&lt;\cdots&lt;tm&#x3D;T_2$,</p>
<p>$\sum (\log\frac{S(t_{j+1})}{S(t_j)})^2&#x3D;\sigma^2\sum(W(t_{j+1})-W(t_j))^2+(\alpha-\frac{1}{2}\sigma^2)^2\sum(t_{j+1}-t_j)^2+2\sigma (\alpha-\frac{1}{2}\sigma^2)\sum(W(t_{j+1})-W(t_j))(t_{j+1}-t_j)$</p>
<p>When the maximum step size $\Vert\Pi\Vert&#x3D;\max_{j&#x3D;0,1,\dots,m-1}(t_{j+1}-t_j)$ is small, then the first term is approximately equal to its limit $\sigma^2(T_2-T_1)$,hence, we have:</p>
<p>$\frac{1}{T_2-T_1}\sum(\log\frac{S(t_{j+1})}{S(t_j)})^2\approx\sigma^2$</p>
<h2 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h2><p><strong>Theorem</strong> Let$W(t),t\ge0$, be a Brownian motion and let $\mathcal{F}(t),t\ge0$ Be a filtration for this Brownian motion. Then $W(t)$ is a Markov process</p>
<p>PROOF</p>
<p>We need to show:  $\mathbb{E}[f(W(t))\vert \mathcal{F}(s)]&#x3D;g(W(s))$ $g$ Exists whenever given $0\le s\le t,f$</p>
<p>$\mathbb{E}[f(W(t))\vert\mathcal{F}(s)]&#x3D;\mathbb{E}[f((W(t)-W(s))+W(s))\vert\mathcal{F}(s)]$</p>
<p>$W(t)-W(s)$ is normally distributed with mean zero and variance $t-s$</p>
<p>Replace $W(s)$ with a dummy variable $x$<br>, define $g(x)&#x3D;\mathbb{E}f(W(t)-W(s)+x)$, then </p>
<p>$g(x)&#x3D;\frac{1}{\sqrt{2\pi(t-s)}}\int f(w+x)\exp{-\frac{w^2}{2(t-s)}}dw&#x3D;\frac{1}{\sqrt{2\pi\tau}}\int f(y)\exp{-\frac{(y-x)^2}{2\tau}}dy$</p>
<p>Define the <em>transition density $p(\tau,x,y):&#x3D;\frac{1}{\sqrt{2\pi\tau}}e^{-\frac{(y-x)^2}{2\tau}},\tau&#x3D;t-s$</em></p>
<p>$g(x)&#x3D;\int f(y)p(\tau,x,y)dy$ </p>
<p>And $\mathbb{E}[f(W(t))\vert\mathcal{F}(s)]&#x3D;\int f(y)p(\tau,W(s),y)dy$</p>
<p>Conditioned on the information in $\mathcal{F}(s)$, the conditional density of $W(t)$ is $p(\tau,W(s),y)$. This is a density in the variable $y$, normal with mean $W(s)$ and variance $\tau$. The only information from $\mathcal{F}(s)$ that is relevant is the value of $W(s)$</p>
<h2 id="First-Passage-Time-Distribution"><a href="#First-Passage-Time-Distribution" class="headerlink" title="First Passage Time Distribution"></a>First Passage Time Distribution</h2><p>exponential martingale corresponding to $\sigma$: $Z(t)&#x3D;\exp{\sigma W(t)-\frac{1}{2}\sigma^2t}$</p>
<p><strong>Theorem</strong> Exponential martingale  Let $W(t),t\ge 0$, be a Brownian motion with a filtration $\mathcal{F}(t),t\ge 0$, and let $\sigma$ be a constant, thew process $Z(t)$ is a martingale</p>
<p>PROOF</p>
<p>$\mathbb{E}[Z(t)\vert\mathcal{F}(s)]&#x3D;\mathbb{E}[\exp{\sigma W(t)-\frac{1}{2}\sigma^2t}\vert \mathcal{F}(s)]&#x3D;\mathbb{E}[\exp{\sigma (W(t)-W(s))}\cdot\exp{\sigma W(s)-\frac{1}{2}\sigma^2t}\vert \mathcal{F}(s)]&#x3D;\exp{\sigma W(s)-\frac{1}{2}\sigma^2t}\cdot\mathbb{E}[\exp{\sigma (W(t)-W(s))}]$</p>
<p>$W(t)-W(s)$ is a normal distribution with mean zero and variance $\sigma$ so $\mathbb{E}[\exp{\sigma (W(t)-W(s))}]&#x3D;\frac{1}{2}\sigma^2(t-s)$</p>
<p>$\mathbb{E}[Z(t)\vert\mathcal{F}(s)]&#x3D;Z(s)$</p>
<p>Let $m$ be a real number, and define the first passage time to level $m$: $\tau_m&#x3D;\min{t\ge 0;W(t)&#x3D;m}$ if the BM never reaches the level $m$, we set $\tau_m&#x3D;\infty$. A martingale that is stopped at a stopping time is still martingale and thus must have constant expectation. So:</p>
<p>$1&#x3D;Z(0)&#x3D;\mathbb{E}Z(t\land\tau_m)&#x3D;\mathbb{E}[\exp{\sigma W(t\land\tau_m)-\frac{1}{2}\sigma^2(t\land\tau_m)}]$ $t\land\tau_m$ means the minimum of $t$ and $\tau_m$</p>
<p>If $\tau_m&lt;\infty$, the term $\exp{-\frac{1}{2}\sigma^2(t\land\tau_m)}$ is equal to $\exp{-\frac{1}{2}\sigma^2\tau_m}$ for large enough $t$. $\tau_m&#x3D;\infty$, the result converges to zero. So: </p>
<p>$\lim_{t\to\infty}\exp{\sigma W(t\land\tau_m)-\frac{1}{2}\sigma^2(t\land\tau_m)}&#x3D;\mathbb{I}_\exp{\sigma m-\frac{1}{2}\sigma^2\tau_m\}$</p>
<p>now we can obtain:</p>
<p>$1&#x3D;\mathbb{E}[\mathbb{I}_\exp{\sigma m-\frac{1}{2}\sigma^2\tau_m}]$</p>
<p>$\mathbb{E}[\mathbb{I}_\exp{-\frac{1}{2}\sigma^2\tau_m}]&#x3D;e^{-\sigma m}$</p>
<p>when $\sigma\to0$, we get $\mathbb{P}{\tau_m&lt;\infty}&#x3D;1$</p>
<p>$\tau_m$ is finite almost surely, so we may drop the indicator to obtain:</p>
<p>$\mathbb{E}[\exp{-\frac{1}{2}\sigma^2\tau_m}]&#x3D;e^{-\sigma m}$</p>
<p><strong>Theorem</strong> For $m\in \mathbb{R}$, the first passage time of Brownian motion to level $m$ is finite almost surely, and the Laplace transform of its distribution is given by </p>
<p>$\mathbb{E}e^{-\alpha \tau_m}&#x3D;e^{-\vert m \vert\sqrt{2\alpha}}$  for all $\alpha&gt;0$</p>
<p>differentiation: $\mathbb{E}[\tau_me^{-\alpha\tau_m}]&#x3D;\frac{\vert m\vert}{\sqrt{2\alpha}}e^{-\vert m\vert\sqrt{2\alpha}}$ for all $\alpha&gt;0$</p>
<p>let $\alpha\to 0$ get obtain $\mathbb{E}\tau_m&#x3D;\infty$ so long as $m\neq0$</p>
<h2 id="Reflection-Principle"><a href="#Reflection-Principle" class="headerlink" title="Reflection Principle"></a>Reflection Principle</h2><h3 id="Reflection-Equality"><a href="#Reflection-Equality" class="headerlink" title="Reflection Equality"></a>Reflection Equality</h3><p>for each Brownian motion path that reaches level m prior to time t but is at a level w below m at time t, there is a â€œreflected pathâ€ that is at level $2m-w$ at time $t$. This reflected path is constructed by switching the up and down moves of the Brownian motion from time $\tau_m$onward.</p>
<p><strong>Reflection equality</strong></p>
<p>$\mathbb{P}{\tau_m\le t,W(t)\le w}&#x3D;\mathbb{P}{W(t)\ge 2m-w},w\le m,m&gt;0$</p>
<h3 id="First-Passage-Time-Distribution-1"><a href="#First-Passage-Time-Distribution-1" class="headerlink" title="First Passage Time Distribution"></a>First Passage Time Distribution</h3><p><strong>Theorem</strong> For all $m\neq0$, the random variable $\tau_m$ has cumulative distribution function:</p>
<p>$\mathbb{P}{\tau_m\le t}&#x3D;\frac{2}{\sqrt{2\pi}}\int_{\frac{\vert m\vert}{\sqrt{t}}}^\infty e^{-\frac{y^2}{2}}dy$</p>
<p>$f_{\tau_m}(t)&#x3D;\frac{d}{dt}\mathbb{P}&#x3D;\frac{\vert m\vert}{t\sqrt{2\pi t}}e^{-\frac{m^2}{2t}}$</p>
<p>PROOF</p>
<p>Use the reflection equality, we obtain</p>
<p>$\mathbb{P}{\tau_m\le t,W(t)\le m}&#x3D;\mathbb{P}{W(t)\ge m}$</p>
<p>if $W(t)\ge m$, then we are guaranteed that $\tau_m\le t$. </p>
<p>$\mathbb{P}{\tau_m\le t,W(t)\ge m}&#x3D;\mathbb{P}{W(t)\ge m}$</p>
<p>so,$\mathbb{P}{\tau_m\le t}&#x3D;2\mathbb{P}{W(t)\ge m}&#x3D;\frac{2}{\sqrt{2\pi t}}\int_{m}^\infty e^{-\frac{x^2}{2t}}dx&#x3D;\frac{2}{\sqrt{2\pi}}\int_{\frac{\vert m\vert}{\sqrt{t}}}^\infty e^{-\frac{y^2}{2}}dy$</p>
<h3 id="Distribution-of-Brownian-Motion-and-Its-Maximum"><a href="#Distribution-of-Brownian-Motion-and-Its-Maximum" class="headerlink" title="Distribution of Brownian Motion and Its Maximum"></a>Distribution of Brownian Motion and Its Maximum</h3><p>define the <strong>maximum to date</strong> for Brownian motion</p>
<p>$M(t)&#x3D;\max_{0\le s\le t}W(s)$</p>
<p>Use the reflection equality, we can obtain the joint distribution of $W(t)$<br> and $M(t)$</p>
<p><strong>Theorem</strong> For $t&gt;0$, the joint density of $(M(t),W(t))$ is</p>
<p>$f_{M(t),W(t)}(m,w)&#x3D;\frac{2(2m-w)}{t\sqrt{2\pi t}}e^{-\frac{(2m-w)^2}{2t}}$</p>
<p>PROOF</p>
<p>$\mathbb{P}{M(t)\ge m,W(t)\le w}&#x3D;\int_m^\infty\int_{-\infty}^wf_{M(t),W(t)}(x,y)dydx$</p>
<p>$\mathbb{P}{W(t)\ge 2m-w}&#x3D;\frac{1}{\sqrt{2\pi t}}\int_{2m-w}^\infty e^{-\frac{z^2}{2t}}dz$</p>
<p>From the reflection equality, </p>
<p>$\frac{1}{\sqrt{2\pi t}}\int_{2m-w}^\infty e^{-\frac{z^2}{2t}}dz&#x3D; \int_m^\infty\int_{-\infty}^wf_{M(t),W(t)}(x,y)dydx$</p>
<p>Differentiate with respect to $m$</p>
<p>$-\int_{-\infty}^wf_{M(t),W(t)}(m,y)dy&#x3D;-\frac{2}{\sqrt{2\pi t}}e^{-\frac{(2m-w)^2}{2t}}$</p>
<p>With respect to $w$, then we obtain the distribution</p>
<p><strong>Corollary</strong> </p>
<p>The conditional distribution of $M(t)$ given $W(t)&#x3D;w$ is $f_{M(t)\vert W(t)}(m\vert w)&#x3D;\frac{2(2m-w)}{t}e^{-\frac{2m(m-w)}{t}}$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ryan Liu</p>
  <div class="site-description" itemprop="description">ä¸€ä¸ªå’¸é±¼çš„ç¬”è®°</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">1</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:ruoyanlau@outlook.com" title="E-Mail â†’ mailto:ruoyanlau@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/ye-yu-ling-ling-88" title="Hu â†’ https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;ye-yu-ling-ling-88" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Hu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/baixu-liu-ryan" title="LinkedIn â†’ https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;baixu-liu-ryan" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>LinkedIn</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ryan Liu</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
